{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVAcMUT8P9B-"
      },
      "outputs": [],
      "source": [
        "# !pip install mmcv\n",
        "# !pip install argparse\n",
        "\n",
        "# import mmcv\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import copy\n",
        "import time\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import multiprocessing as mul\n",
        "import uuid\n",
        "import psutil\n",
        "import time\n",
        "import csv\n",
        "import math\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "from functools import wraps\n",
        "from collections import OrderedDict\n",
        "from inspect import getfullargspec\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose\n",
        "from sklearn.metrics import accuracy_score, roc_curve, confusion_matrix\n",
        "from scipy.interpolate import make_interp_spline\n",
        "from functools import partial\n",
        "# from mmcv import scandir\n",
        "\n",
        "from scipy.stats import wasserstein_distance\n",
        "from skimage.metrics import normalized_root_mse\n",
        "\n",
        "from __future__ import print_function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "pE4hpukDQCFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(os.getcwd())\n",
        "\n",
        "\n",
        "class Parser(object):\n",
        "    def __init__(self):\n",
        "        self.parser = argparse.ArgumentParser(add_help=False)\n",
        "\n",
        "\n",
        "        self.parser.add_argument('--task', default='congestion_gpdl')\n",
        "        self.parser.add_argument('--save_path', default='work_dir/congestion_gpdl/')\n",
        "        self.parser.add_argument('--pretrained', default=None)\n",
        "        self.parser.add_argument('--max_iters', type=int, default=200000)\n",
        "        self.parser.add_argument('--plot_roc', action='store_true')\n",
        "        self.parser.add_argument('--arg_file', default=None)\n",
        "        self.parser.add_argument('--cpu', action='store_true')\n",
        "\n",
        "    def parse_args(self):\n",
        "        args, _ = self.parser.parse_known_args()\n",
        "\n",
        "        self._add_task_args(args.task)\n",
        "\n",
        "        args, _ = self.parser.parse_known_args()\n",
        "        return args\n",
        "\n",
        "    def _add_task_args(self, task):\n",
        "        if task == 'congestion_gpdl':\n",
        "            self.parser.add_argument('--dataroot', default='../../training_set/congestion')\n",
        "            self.parser.add_argument('--ann_file_train', default='./files/train_N28.csv')\n",
        "            self.parser.add_argument('--ann_file_test', default='./files/test_N28.csv')\n",
        "            self.parser.add_argument('--dataset_type', default='CongestionDataset')\n",
        "            self.parser.add_argument('--batch_size', type=int, default=16)\n",
        "            self.parser.add_argument('--aug_pipeline', default=['Flip'])\n",
        "\n",
        "            self.parser.add_argument('--model_type', default='GPDL')\n",
        "            self.parser.add_argument('--in_channels', type=int, default=3)\n",
        "            self.parser.add_argument('--out_channels', type=int, default=1)\n",
        "            self.parser.add_argument('--lr', type=float, default=2e-4)\n",
        "            self.parser.add_argument('--weight_decay', type=float, default=0)\n",
        "            self.parser.add_argument('--loss_type', default='MSELoss')\n",
        "            self.parser.add_argument('--eval_metric', default=['NRMS', 'SSIM', 'EMD'])\n",
        "\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task: {task}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Flip:\n",
        "    _directions = ['horizontal', 'vertical']\n",
        "\n",
        "    def __init__(self, keys=['feature', 'label'], flip_ratio=0.5, direction='horizontal', **kwargs):\n",
        "        if direction not in self._directions:\n",
        "            raise ValueError(f'Direction {direction} is not supported.'\n",
        "                             f'Currently support ones are {self._directions}')\n",
        "        self.keys = keys\n",
        "        self.flip_ratio = flip_ratio\n",
        "        self.direction = direction\n",
        "\n",
        "    def __call__(self, results):\n",
        "        flip = np.random.random() < self.flip_ratio\n",
        "\n",
        "        if flip:\n",
        "            for key in self.keys:\n",
        "                if isinstance(results[key], list):\n",
        "                    for v in results[key]:\n",
        "                        mmcv.imflip_(v, self.direction)\n",
        "                else:\n",
        "                    mmcv.imflip_(results[key], self.direction)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "class Rotation:\n",
        "    def __init__(self, keys=['feature', 'label'], axis=(0,1), rotate_ratio=0.5, **kwargs):\n",
        "        self.keys = keys\n",
        "        self.axis = {k:axis for k in keys} if isinstance(axis, tuple) else axis\n",
        "        self.rotate_ratio = rotate_ratio\n",
        "        self.direction = [0, -1, -2, -3]\n",
        "\n",
        "    def __call__(self, results):\n",
        "        rotate = np.random.random() < self.rotate_ratio\n",
        "\n",
        "        if rotate:\n",
        "            rotate_angle = self.direction[int(np.random.random()/(10.0/3.0))+1]\n",
        "            for key in self.keys:\n",
        "                if isinstance(results[key], list):\n",
        "                    for v in results[key]:\n",
        "                        results[key] = np.ascontiguousarray(np.rot90(v, rotate_angle, axes=self.axis[key]))\n",
        "                else:\n",
        "                    results[key] = np.ascontiguousarray(np.rot90(results[key], rotate_angle, axes=self.axis[key]))\n",
        "\n",
        "        return results\n",
        "\n",
        "class IterLoader:\n",
        "    def __init__(self, dataloader):\n",
        "        self._dataloader = dataloader\n",
        "        self.iter_loader = iter(self._dataloader)\n",
        "\n",
        "    def __next__(self):\n",
        "        try:\n",
        "            data = next(self.iter_loader)\n",
        "        except StopIteration:\n",
        "            time.sleep(2)\n",
        "            self.iter_loader = iter(self._dataloader)\n",
        "            data = next(self.iter_loader)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._dataloader)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "class CongestionDataset(object):\n",
        "\n",
        "    def __init__(self, ann_file, dataroot, pipeline=None, test_mode=False):\n",
        "        self.ann_file = ann_file\n",
        "        self.dataroot = dataroot\n",
        "        self.test_mode = test_mode\n",
        "        self.pipeline = Compose(pipeline) if pipeline else None\n",
        "        self.data_infos = self.load_annotations()\n",
        "\n",
        "    def load_annotations(self):\n",
        "        data_infos = []\n",
        "        with open(self.ann_file, 'r') as fin:\n",
        "            for line in fin:\n",
        "                feature, label = line.strip().split(',')\n",
        "                data_infos.append(dict(\n",
        "                    feature_path=osp.join(self.dataroot, feature),\n",
        "                    label_path=osp.join(self.dataroot, label)\n",
        "                ))\n",
        "        return data_infos\n",
        "\n",
        "    def prepare_data(self, idx):\n",
        "        results = copy.deepcopy(self.data_infos[idx])\n",
        "        results['feature'] = np.load(results['feature_path'])\n",
        "        results['label'] = np.load(results['label_path'])\n",
        "\n",
        "        if self.pipeline:\n",
        "            results = self.pipeline(results)\n",
        "\n",
        "        feature = results['feature'].transpose(2, 0, 1).astype(np.float32)\n",
        "        label = results['label'].transpose(2, 0, 1).astype(np.float32)\n",
        "\n",
        "        return feature, label, results['label_path']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_infos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.prepare_data(idx)\n",
        "\n",
        "\n",
        "def build_dataset(opt):\n",
        "    opt = opt.copy()\n",
        "\n",
        "    aug_methods = {\n",
        "        'Flip': Flip(),\n",
        "        'Rotation': Rotation(**opt)\n",
        "    }\n",
        "\n",
        "    pipeline = None\n",
        "    if not opt.get('test_mode', False) and 'aug_pipeline' in opt:\n",
        "        pipeline = [aug_methods[name] for name in opt.pop('aug_pipeline')]\n",
        "\n",
        "    dataset = CongestionDataset(ann_file=opt.pop('ann_file'),dataroot=opt.pop('dataroot'),pipeline=pipeline,test_mode=opt.get('test_mode', False))\n",
        "\n",
        "    if opt.get('test_mode', False):\n",
        "        return DataLoader(dataset=dataset,batch_size=1,shuffle=False,num_workers=1)\n",
        "    else:\n",
        "        return IterLoader(\n",
        "            DataLoader(dataset=dataset,batch_size=opt.pop('batch_size'),shuffle=True,drop_last=True,num_workers=1,pin_memory=True)\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "amD0IEBujM6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generation_init_weights(module):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1\n",
        "                                    or classname.find('Linear') != -1):\n",
        "\n",
        "            if hasattr(m, 'weight') and m.weight is not None:\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    module.apply(init_func)\n",
        "\n",
        "def load_state_dict(module, state_dict, strict=False, logger=None):\n",
        "    unexpected_keys = []\n",
        "    all_missing_keys = []\n",
        "    err_msg = []\n",
        "\n",
        "    metadata = getattr(state_dict, '_metadata', None)\n",
        "    state_dict = state_dict.copy()\n",
        "    if metadata is not None:\n",
        "        state_dict._metadata = metadata\n",
        "\n",
        "    def load(module, prefix=''):\n",
        "        local_metadata = {} if metadata is None else metadata.get(\n",
        "            prefix[:-1], {})\n",
        "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
        "                                     all_missing_keys, unexpected_keys,\n",
        "                                     err_msg)\n",
        "        for name, child in module._modules.items():\n",
        "            if child is not None:\n",
        "                load(child, prefix + name + '.')\n",
        "\n",
        "    load(module)\n",
        "    load = None\n",
        "\n",
        "    missing_keys = [\n",
        "        key for key in all_missing_keys if 'num_batches_tracked' not in key\n",
        "    ]\n",
        "\n",
        "    if unexpected_keys:\n",
        "        err_msg.append('unexpected key in source '\n",
        "                       f'state_dict: {\", \".join(unexpected_keys)}\\n')\n",
        "    if missing_keys:\n",
        "        err_msg.append(\n",
        "            f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n",
        "\n",
        "    if len(err_msg) > 0:\n",
        "        err_msg.insert(\n",
        "            0, 'The model and loaded state dict do not match exactly\\n')\n",
        "        err_msg = '\\n'.join(err_msg)\n",
        "        if strict:\n",
        "            raise RuntimeError(err_msg)\n",
        "        elif logger is not None:\n",
        "            logger.warning(err_msg)\n",
        "        else:\n",
        "            print(err_msg)\n",
        "    return missing_keys\n",
        "\n",
        "class conv(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=True):\n",
        "        super(conv, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class upconv(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super(upconv, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "                nn.ConvTranspose2d(dim_in, dim_out, 4, 2, 1),\n",
        "                nn.InstanceNorm2d(dim_out, affine=True),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim=3, out_dim=32):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.c1 = conv(in_dim, 32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.c2 = conv(32, 64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.c3 = nn.Sequential(\n",
        "                nn.Conv2d(64, out_dim, 3, 1, 1),\n",
        "                nn.BatchNorm2d(out_dim),\n",
        "                nn.Tanh()\n",
        "                )\n",
        "\n",
        "    def init_weights(self):\n",
        "        generation_init_weights(self)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        h1 = self.c1(input)\n",
        "        h2 = self.pool1(h1)\n",
        "        h3 = self.c2(h2)\n",
        "        h4 = self.pool2(h3)\n",
        "        h5 = self.c3(h4)\n",
        "        return h5, h2  # shortpath from 2->7\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, out_dim=2, in_dim=32):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = conv(in_dim, 32)\n",
        "        self.upc1 = upconv(32, 16)\n",
        "        self.conv2 = conv(16, 16)\n",
        "        self.upc2 = upconv(32+16, 4)\n",
        "        self.conv3 =  nn.Sequential(\n",
        "                nn.Conv2d(4, out_dim, 3, 1, 1),\n",
        "                nn.Sigmoid()\n",
        "                )\n",
        "\n",
        "    def init_weights(self):\n",
        "        generation_init_weights(self)\n",
        "\n",
        "    def forward(self, input):\n",
        "        feature, skip = input\n",
        "        d1 = self.conv1(feature)\n",
        "        d2 = self.upc1(d1)\n",
        "        d3 = self.conv2(d2)\n",
        "        d4 = self.upc2(torch.cat([d3, skip], dim=1))\n",
        "        output = self.conv3(d4)  # shortpath from 2->7\n",
        "        return output\n",
        "\n",
        "\n",
        "class GPDL(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=3,\n",
        "                 out_channels=1,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(in_dim=in_channels)\n",
        "        self.decoder = Decoder(out_dim=out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.decoder(x)\n",
        "\n",
        "    def init_weights(self, pretrained=None, pretrained_transfer=None, strict=False, **kwargs):\n",
        "        if isinstance(pretrained, str):\n",
        "            new_dict = OrderedDict()\n",
        "            weight = torch.load(pretrained, map_location='cpu')['state_dict']\n",
        "            for k in weight.keys():\n",
        "                new_dict[k] = weight[k]\n",
        "            load_state_dict(self, new_dict, strict=strict, logger=None)\n",
        "        elif pretrained is None:\n",
        "            generation_init_weights(self)\n",
        "        else:\n",
        "            raise TypeError(\"'pretrained' must be a str or None. \"\n",
        "                            f'But received {type(pretrained)}.')\n",
        "def build_model(opt):\n",
        "    model = GPDL()\n",
        "    model.init_weights(**opt)\n",
        "    if opt['test_mode']:\n",
        "        model.eval()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "6-FbssPIYx-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mkdir_or_exist(dir_name, mode=0o777):\n",
        "    if dir_name == '':\n",
        "        return\n",
        "    dir_name = osp.expanduser(dir_name)\n",
        "    os.makedirs(dir_name, mode=mode, exist_ok=True)\n",
        "\n",
        "\n",
        "def input_converter(apply_to=None):\n",
        "    def input_converter_wrapper(old_func):\n",
        "        @wraps(old_func)\n",
        "        def new_func(*args, **kwargs):\n",
        "            args_info = getfullargspec(old_func)\n",
        "            args_to_cast = args_info.args if apply_to is None else apply_to\n",
        "            new_args = []\n",
        "            if args:\n",
        "                arg_names = args_info.args[:len(args)]\n",
        "                for i, arg_name in enumerate(arg_names):\n",
        "                    if arg_name in args_to_cast:\n",
        "                        new_args.append(tensor2img(args[i]))\n",
        "                    else:\n",
        "                        new_args.append(args[i])\n",
        "\n",
        "            return old_func(*new_args)\n",
        "        return new_func\n",
        "\n",
        "    return input_converter_wrapper\n",
        "\n",
        "\n",
        "@input_converter(apply_to=('img1', 'img2'))\n",
        "def psnr(img1, img2, crop_border=0):\n",
        "    assert img1.shape == img2.shape, (\n",
        "        f'Image shapes are different: {img1.shape}, {img2.shape}.')\n",
        "\n",
        "    img1, img2 = img1.astype(np.float32), img2.astype(np.float32)\n",
        "    if crop_border != 0:\n",
        "        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, None]\n",
        "        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, None]\n",
        "\n",
        "    mse_value = np.mean((img1 - img2)**2)\n",
        "    if mse_value == 0:\n",
        "        return float('inf')\n",
        "    return 20. * np.log10(255. / np.sqrt(mse_value))\n",
        "\n",
        "\n",
        "def _ssim(img1, img2):\n",
        "    C1 = (0.01 * 255)**2\n",
        "    C2 = (0.03 * 255)**2\n",
        "\n",
        "    img1 = img1.astype(np.float64)\n",
        "    img2 = img2.astype(np.float64)\n",
        "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
        "    window = np.outer(kernel, kernel.transpose())\n",
        "\n",
        "    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]\n",
        "    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n",
        "    mu1_sq = mu1**2\n",
        "    mu2_sq = mu2**2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
        "    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
        "    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
        "\n",
        "    ssim_map = ((2 * mu1_mu2 + C1) *\n",
        "                (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
        "                                       (sigma1_sq + sigma2_sq + C2))\n",
        "    return ssim_map.mean()\n",
        "\n",
        "\n",
        "@input_converter(apply_to=('img1', 'img2'))\n",
        "def ssim(img1, img2, crop_border=0):\n",
        "    assert img1.shape == img2.shape, (\n",
        "        f'Image shapes are different: {img1.shape}, {img2.shape}.')\n",
        "    if crop_border != 0:\n",
        "        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, None]\n",
        "        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, None]\n",
        "\n",
        "    ssims = []\n",
        "    for i in range(img1.shape[2]):\n",
        "        ssims.append(_ssim(img1[..., i], img2[..., i]))\n",
        "    return np.array(ssims).mean()\n",
        "\n",
        "\n",
        "@input_converter(apply_to=('img1', 'img2'))\n",
        "def nrms(img1, img2, crop_border=0):\n",
        "    assert img1.shape == img2.shape, (\n",
        "        f'Image shapes are different: {img1.shape}, {img2.shape}.')\n",
        "\n",
        "    if crop_border != 0:\n",
        "        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, None]\n",
        "        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, None]\n",
        "\n",
        "    nrmse_value = normalized_root_mse(img1.flatten(), img2.flatten(),normalization='min-max')\n",
        "    if math.isinf(nrmse_value):\n",
        "        return 0.05\n",
        "    return nrmse_value\n",
        "\n",
        "\n",
        "\n",
        "def get_histogram(img):\n",
        "    h, w = img.shape\n",
        "    hist = [0.0] * 256\n",
        "    for i in range(h):\n",
        "        for j in range(w):\n",
        "            hist[img[i, j]] += 1\n",
        "    return np.array(hist) / float(h * w)\n",
        "\n",
        "\n",
        "def normalize_exposure(img):\n",
        "    img = img.astype(int)\n",
        "    hist = get_histogram(img)\n",
        "    cdf = np.array([sum(hist[:i+1]) for i in range(len(hist))])\n",
        "    sk = np.uint8(255 * cdf)\n",
        "    height, width = img.shape\n",
        "    normalized = np.zeros_like(img)\n",
        "    for i in range(0, height):\n",
        "        for j in range(0, width):\n",
        "            normalized[i, j] = sk[img[i, j]]\n",
        "    return normalized.astype(int)\n",
        "\n",
        "\n",
        "@input_converter(apply_to=('img1', 'img2'))\n",
        "def emd(img1, img2, crop_border=0):\n",
        "    assert img1.shape == img2.shape, (\n",
        "        f'Image shapes are different: {img1.shape}, {img2.shape}.')\n",
        "\n",
        "    if crop_border != 0:\n",
        "        img1 = img1[crop_border:-crop_border, crop_border:-crop_border, None]\n",
        "        img2 = img2[crop_border:-crop_border, crop_border:-crop_border, None]\n",
        "\n",
        "    img1 = normalize_exposure(np.squeeze(img1, axis = 2))\n",
        "    img2 = normalize_exposure(np.squeeze(img2, axis = 2))\n",
        "    hist_1 = get_histogram(img1)\n",
        "    hist_2 = get_histogram(img2)\n",
        "\n",
        "    emd_value = wasserstein_distance(hist_1, hist_2)\n",
        "    return emd_value\n",
        "\n",
        "def tpr(tp, fn):\n",
        "    return tp/(tp+fn)\n",
        "\n",
        "def fpr(fp, tn):\n",
        "    return fp/(fp+tn)\n",
        "\n",
        "def precision(tp, fp):\n",
        "    return tp/(tp+fp)\n",
        "\n",
        "def calculate_all(csv_path):\n",
        "    tpr_sum_List = []\n",
        "    fpr_sum_List = []\n",
        "    precision_sum_List = []\n",
        "    threshold_remain_list = []\n",
        "    num = 0\n",
        "    tpr_sum = 0\n",
        "    fpr_sum = 0\n",
        "    precision_sum = 0\n",
        "\n",
        "    csv_file = open(os.path.join(csv_path), 'r')\n",
        "\n",
        "    first_flag = False\n",
        "    for line in csv_file:\n",
        "        threshold, idx, tn, fp, fn, tp = line.strip().split(',')\n",
        "        if threshold not in threshold_remain_list:\n",
        "            if first_flag:\n",
        "                if num !=0:\n",
        "                    tpr_sum_List.append(tpr_sum/num)\n",
        "                    fpr_sum_List.append(fpr_sum/num)\n",
        "                    precision_sum_List.append(precision_sum/num)\n",
        "            threshold_remain_list.append(threshold)\n",
        "            tpr_sum = 0\n",
        "            fpr_sum = 0\n",
        "            precision_sum = 0\n",
        "            num = 0\n",
        "            first_flag = True\n",
        "\n",
        "        if int(fp)==0 and int(tn)==0:\n",
        "            continue\n",
        "        elif int(tp)==0 and int(fn)==0:\n",
        "            continue\n",
        "        elif int(tp)==0 and int(fp)==0:\n",
        "            continue\n",
        "        else:\n",
        "            tpr_sum += tpr(int(tp), int(fn))\n",
        "            fpr_sum += fpr(int(fp), int(tn))\n",
        "            precision_sum += precision(int(tp), int(fp))\n",
        "            num += 1\n",
        "    if num !=0:\n",
        "        tpr_sum_List.append(tpr_sum/num)\n",
        "        fpr_sum_List.append(fpr_sum/num)\n",
        "        precision_sum_List.append(precision_sum/num)\n",
        "\n",
        "\n",
        "    return tpr_sum_List, fpr_sum_List, precision_sum_List\n",
        "\n",
        "\n",
        "def calculated_score(threshold_idx=None,\n",
        "                     temp_path=None,\n",
        "                     label_path=None,\n",
        "                     save_path=None,\n",
        "                     threshold_label=None,\n",
        "                     preds=None):\n",
        "    file = open(os.path.join(temp_path, f'tpr_fpr_{threshold_idx}.csv'),'w')\n",
        "    f_csv = csv.writer(file, delimiter=',')\n",
        "    for idx, pred in enumerate(preds):\n",
        "        target_test = np.load(os.path.join(label_path, pred)).reshape(-1)\n",
        "        target_probabilities = np.load(os.path.join(save_path, 'test_result', pred)).reshape(-1)\n",
        "\n",
        "        target_test[target_test>=threshold_label] = 1\n",
        "        target_test[target_test<threshold_label] = 0\n",
        "\n",
        "        target_probabilities[target_probabilities>=threshold_idx] = 1\n",
        "        target_probabilities[target_probabilities<threshold_idx] = 0\n",
        "\n",
        "        if np.sum(target_probabilities == 0)==0 and np.sum(target_test == 0)==0:\n",
        "            tp = 256*256\n",
        "            tn, fn, fp = 0,0,0\n",
        "        elif np.sum(target_probabilities == 1)==0 and np.sum(target_test == 1)==0:\n",
        "            tn = 256*256\n",
        "            tp, fn, fp = 0,0,0\n",
        "        else:\n",
        "            tn, fp, fn, tp = confusion_matrix(target_test, target_probabilities).ravel()\n",
        "\n",
        "        f_csv.writerow([str(threshold_idx)]+[str(i) for i in [idx, tn, fp, fn, tp]])\n",
        "\n",
        "\n",
        "    print(f'{threshold_idx}-done')\n",
        "\n",
        "def multi_process_score(out_name=None, threshold=0.0, label_path=None, save_path=None):\n",
        "    uid = str(uuid.uuid4())\n",
        "    suid = ''.join(uid.split('-'))\n",
        "    temp_path = f'./{suid}'\n",
        "\n",
        "    psutil.cpu_percent(None)\n",
        "    time.sleep(0.5)\n",
        "    pool = mul.Pool(int(mul.cpu_count()*(1-psutil.cpu_percent(None)/100.0)))\n",
        "\n",
        "    preds = scandir(os.path.join(save_path, 'test_result'), suffix='npy', recursive=True)\n",
        "    preds = [v for v in preds]\n",
        "\n",
        "    if not os.path.exists(temp_path):\n",
        "        os.makedirs(temp_path)\n",
        "\n",
        "    threshold_list = np.linspace(0, 1, endpoint=False, num=200)\n",
        "\n",
        "    calculated_score_parital = partial(calculated_score, temp_path=temp_path,\n",
        "                                        label_path=label_path, save_path=save_path, threshold_label=threshold, preds=preds)\n",
        "    rel = pool.map(calculated_score_parital, threshold_list)\n",
        "\n",
        "    print(f'{suid}')\n",
        "\n",
        "    for list_i in threshold_list:\n",
        "        fr=open(os.path.join(temp_path, f'tpr_fpr_{list_i}.csv'), 'r').read()\n",
        "        with open(os.path.join(temp_path, f'{out_name}'), 'a') as f:\n",
        "            f.write(fr)\n",
        "        f.close()\n",
        "\n",
        "\n",
        "    print('copying')\n",
        "    os.system('cp {} {}'.format(os.path.join(temp_path, f'{out_name}'), os.path.join(os.path.join(os.getcwd(), save_path), f'{out_name}')))\n",
        "\n",
        "    print('remove temp files')\n",
        "    os.system(f'rm -rf {temp_path}')\n",
        "\n",
        "def get_sorted_list(fpr_sum_List,tpr_sum_List):\n",
        "    fpr_list = []\n",
        "    tpr_list = []\n",
        "    for i, j in zip(fpr_sum_List, tpr_sum_List):\n",
        "        if i not in fpr_list:\n",
        "            fpr_list.append(i)\n",
        "            tpr_list.append(j)\n",
        "\n",
        "    fpr_list.reverse()\n",
        "    tpr_list.reverse()\n",
        "    fpr_list, tpr_list = zip(*sorted(zip(fpr_list, tpr_list)))\n",
        "    return fpr_list, tpr_list\n",
        "\n",
        "\n",
        "def roc_prc(save_path):\n",
        "    tpr_sum_List, fpr_sum_List, precision_sum_List = calculate_all(os.path.join(os.getcwd(), save_path, 'roc_prc.csv'))\n",
        "\n",
        "    fpr_list, tpr_list = get_sorted_list(fpr_sum_List,tpr_sum_List)\n",
        "    fpr_list = list(fpr_list)\n",
        "    fpr_list.extend([1])\n",
        "\n",
        "    tpr_list = list(tpr_list)\n",
        "    tpr_list.extend([1])\n",
        "\n",
        "    roc_numerator = 0\n",
        "    for i in range(len(tpr_list)-1):\n",
        "        roc_numerator += (tpr_list[i]+tpr_list[i+1])*(fpr_list[i+1]-fpr_list[i])/2\n",
        "\n",
        "    tpr_list, p_list = get_sorted_list(tpr_sum_List, precision_sum_List)\n",
        "    x_smooth = np.linspace(0, 1, 25)\n",
        "    y_smooth = make_interp_spline(tpr_list, p_list, k=3)(x_smooth)\n",
        "\n",
        "    prc_numerator = 0\n",
        "    for i in range(len(y_smooth)-1):\n",
        "        prc_numerator += (y_smooth[i]+y_smooth[i+1])*(x_smooth[i+1]-x_smooth[i])/2\n",
        "\n",
        "    return roc_numerator, prc_numerator\n",
        "\n",
        "\n",
        "\n",
        "def tensor2img(tensor, out_type=np.uint8, min_max=(0, 1)):\n",
        "    if not (torch.is_tensor(tensor) or\n",
        "            (isinstance(tensor, list)\n",
        "             and all(torch.is_tensor(t) for t in tensor))):\n",
        "        raise TypeError(\n",
        "            f'tensor or list of tensors expected, got {type(tensor)}')\n",
        "\n",
        "    if torch.is_tensor(tensor):\n",
        "        tensor = [tensor]\n",
        "    result = []\n",
        "    for _tensor in tensor:\n",
        "        _tensor = _tensor.squeeze(0).squeeze(0)\n",
        "        _tensor = _tensor.float().detach().cpu().clamp_(*min_max)\n",
        "        _tensor = (_tensor - min_max[0]) / (min_max[1] - min_max[0])\n",
        "        n_dim = _tensor.dim()\n",
        "\n",
        "        if n_dim == 3:\n",
        "            img_np = _tensor.numpy()\n",
        "            img_np = np.transpose(img_np[:, :, :], (2, 0, 1))\n",
        "            # img_np = np.transpose(img_np[[2, 1, 0], :, :], (1, 2, 0))\n",
        "        elif n_dim == 2:\n",
        "            img_np = _tensor.numpy()[..., None]\n",
        "        else:\n",
        "            raise ValueError('Only support 4D, 3D or 2D tensor. '\n",
        "                             f'But received with dimension: {n_dim}')\n",
        "        if out_type == np.uint8:\n",
        "            img_np = (img_np * 255.0).round()\n",
        "        img_np = img_np.astype(out_type)\n",
        "        result.append(img_np)\n",
        "    result = result[0] if len(result) == 1 else result\n",
        "    return result\n",
        "\n",
        "\n",
        "def build_metric(metric_name):\n",
        "\n",
        "    return globals()[metric_name.lower()]\n",
        "\n",
        "\n",
        "def build_roc_prc_metric(threshold=None, dataroot=None, ann_file=None, save_path=None, **kwargs):\n",
        "    if ann_file:\n",
        "        with open(ann_file, 'r') as fin:\n",
        "            for line in fin:\n",
        "                if len(line.strip().split(',')) == 2:\n",
        "                    feature, label = line.strip().split(',')\n",
        "                else:\n",
        "                    label = line.strip().split(',')[-1]\n",
        "                break\n",
        "\n",
        "        label_name = label.split('/')[0]\n",
        "    else:\n",
        "        raise FileExistsError\n",
        "    print(os.path.join(dataroot, label_name))\n",
        "    multi_process_score(out_name='roc_prc.csv', threshold=threshold, label_path=os.path.join(dataroot, label_name), save_path=os.path.join('.', save_path))\n",
        "\n",
        "    return roc_prc(save_path)\n"
      ],
      "metadata": {
        "id": "G1E7nTF8tC9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(arg_dict):\n",
        "    arg_dict = arg_dict.copy()\n",
        "\n",
        "    arg_dict['ann_file'] = arg_dict['ann_file_test']\n",
        "    arg_dict['test_mode'] = True\n",
        "    arg_dict[\"out_channels\"] = 1\n",
        "\n",
        "\n",
        "    print('===> Loading datasets')\n",
        "    dataset = build_dataset(arg_dict)\n",
        "\n",
        "    print('===> Building model')\n",
        "    model = build_model(arg_dict)\n",
        "    if not arg_dict['cpu']:\n",
        "        model = model.cuda()\n",
        "    model.eval()\n",
        "\n",
        "    metrics = {k: build_metric(k) for k in arg_dict['eval_metric']}\n",
        "    avg_metrics = {k: 0.0 for k in arg_dict['eval_metric']}\n",
        "\n",
        "    with torch.no_grad(), tqdm(total=len(dataset)) as bar:\n",
        "        for feature, label, label_path in dataset:\n",
        "            if arg_dict['cpu']:\n",
        "                input, target = feature, label\n",
        "            else:\n",
        "                input, target = feature.cuda(), label.cuda()\n",
        "\n",
        "            prediction = model(input)\n",
        "\n",
        "        for metric, metric_func in metrics.items():\n",
        "            avg_metrics[metric] += metric_func(\n",
        "                target.cpu(),\n",
        "                prediction.cpu()\n",
        "            )\n",
        "\n",
        "\n",
        "            bar.update(1)\n",
        "\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"===> Avg. {metric}: {value / len(dataset):.4f}\")\n"
      ],
      "metadata": {
        "id": "PAZY2_Ox_85C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_args = {\n",
        "    \"task\": \"congestion_gpdl\",\n",
        "    \"save_path\": \"work_dir/\",\n",
        "    \"ann_file_test\": \"/content/test_N28.csv\",\n",
        "    \"dataroot\": \"/content/drive/MyDrive/congestion\",\n",
        "    \"batch_size\": 1,\n",
        "    \"cpu\": True,\n",
        "    \"eval_metric\": [\"NRMS\", \"SSIM\"],\n",
        "    \"plot_roc\": False,\n",
        "    \"pretrained\": \"/content/model_iters_200000.pth\",\n",
        "    \"out_channels\": 1,\n",
        "}\n",
        "\n",
        "test(test_args)\n"
      ],
      "metadata": {
        "id": "YBqQF4H4eHp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff0afc1e-2d68-4904-e9f0-c449d98648f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===> Loading datasets\n",
            "===> Building model\n",
            "The model and loaded state dict do not match exactly\n",
            "\n",
            "size mismatch for decoder.conv3.0.weight: copying a param with shape torch.Size([1, 4, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 4, 3, 3]).\n",
            "size mismatch for decoder.conv3.0.bias: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([2]).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/3164 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fBPno1Brv0H2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
