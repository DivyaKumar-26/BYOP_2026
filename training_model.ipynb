{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "jVAcMUT8P9B-"
      },
      "outputs": [],
      "source": [
        "# !pip install mmcv\n",
        "# !pip install argparse\n",
        "\n",
        "# import mmcv\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "import copy\n",
        "import time\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import multiprocessing as mul\n",
        "import uuid\n",
        "import psutil\n",
        "import time\n",
        "import csv\n",
        "import math\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import functools\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import Compose\n",
        "from sklearn.metrics import accuracy_score, roc_curve, confusion_matrix\n",
        "from scipy.interpolate import make_interp_spline\n",
        "from functools import partial\n",
        "# from mmcv import scandir\n",
        "\n",
        "from scipy.stats import wasserstein_distance\n",
        "from skimage.metrics import normalized_root_mse\n",
        "\n",
        "from __future__ import print_function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "pE4hpukDQCFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f82228d8-6703-4534-d1de-c4bda820ad6a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sys.path.append(os.getcwd())\n",
        "\n",
        "\n",
        "class Parser(object):\n",
        "    def __init__(self):\n",
        "        self.parser = argparse.ArgumentParser(add_help=False)\n",
        "\n",
        "\n",
        "        self.parser.add_argument('--task', default='congestion_gpdl')\n",
        "        self.parser.add_argument('--save_path', default='work_dir/congestion_gpdl/')\n",
        "        self.parser.add_argument('--pretrained', default=None)\n",
        "        self.parser.add_argument('--max_iters', type=int, default=200000)\n",
        "        self.parser.add_argument('--plot_roc', action='store_true')\n",
        "        self.parser.add_argument('--arg_file', default=None)\n",
        "        self.parser.add_argument('--cpu', action='store_true')\n",
        "\n",
        "    def parse_args(self):\n",
        "        args, _ = self.parser.parse_known_args()\n",
        "\n",
        "        self._add_task_args(args.task)\n",
        "\n",
        "        args, _ = self.parser.parse_known_args()\n",
        "        return args\n",
        "\n",
        "    def _add_task_args(self, task):\n",
        "        if task == 'congestion_gpdl':\n",
        "            self.parser.add_argument('--dataroot', default='../../training_set/congestion')\n",
        "            self.parser.add_argument('--ann_file_train', default='./files/train_N28.csv')\n",
        "            self.parser.add_argument('--ann_file_test', default='./files/test_N28.csv')\n",
        "            self.parser.add_argument('--dataset_type', default='CongestionDataset')\n",
        "            self.parser.add_argument('--batch_size', type=int, default=16)\n",
        "            self.parser.add_argument('--aug_pipeline', default=['Flip'])\n",
        "\n",
        "            self.parser.add_argument('--model_type', default='GPDL')\n",
        "            self.parser.add_argument('--in_channels', type=int, default=3)\n",
        "            self.parser.add_argument('--out_channels', type=int, default=1)\n",
        "            self.parser.add_argument('--lr', type=float, default=2e-4)\n",
        "            self.parser.add_argument('--weight_decay', type=float, default=0)\n",
        "            self.parser.add_argument('--loss_type', default='MSELoss')\n",
        "            self.parser.add_argument('--eval_metric', default=['NRMS', 'SSIM', 'EMD'])\n",
        "\n",
        "        elif task == 'drc_routenet':\n",
        "            self.parser.add_argument('--dataroot', default='../../training_set/DRC')\n",
        "            self.parser.add_argument('--ann_file_train', default='./files/train_N28.csv')\n",
        "            self.parser.add_argument('--ann_file_test', default='./files/test_N28.csv')\n",
        "            self.parser.add_argument('--dataset_type', default='DRCDataset')\n",
        "            self.parser.add_argument('--batch_size', type=int, default=8)\n",
        "            self.parser.add_argument('--aug_pipeline', default=['Flip'])\n",
        "\n",
        "            self.parser.add_argument('--model_type', default='RouteNet')\n",
        "            self.parser.add_argument('--in_channels', type=int, default=9)\n",
        "            self.parser.add_argument('--out_channels', type=int, default=1)\n",
        "            self.parser.add_argument('--lr', type=float, default=2e-4)\n",
        "            self.parser.add_argument('--weight_decay', type=float, default=1e-4)\n",
        "            self.parser.add_argument('--loss_type', default='MSELoss')\n",
        "            self.parser.add_argument('--eval_metric', default=['NRMS', 'SSIM'])\n",
        "            self.parser.add_argument('--threshold', type=float, default=0.1)\n",
        "\n",
        "        elif task == 'irdrop_mavi':\n",
        "            self.parser.add_argument('--dataroot', default='../../training_set/IR_drop')\n",
        "            self.parser.add_argument('--ann_file_train', default='./files/train_N28.csv')\n",
        "            self.parser.add_argument('--ann_file_test', default='./files/test_N28.csv')\n",
        "            self.parser.add_argument('--dataset_type', default='IRDropDataset')\n",
        "            self.parser.add_argument('--batch_size', type=int, default=2)\n",
        "\n",
        "            self.parser.add_argument('--model_type', default='MAVI')\n",
        "            self.parser.add_argument('--in_channels', type=int, default=1)\n",
        "            self.parser.add_argument('--out_channels', type=int, default=4)\n",
        "            self.parser.add_argument('--lr', type=float, default=2e-4)\n",
        "            self.parser.add_argument('--weight_decay', type=float, default=1e-2)\n",
        "            self.parser.add_argument('--loss_type', default='L1Loss')\n",
        "            self.parser.add_argument('--eval_metric', default=['NRMS', 'SSIM'])\n",
        "            self.parser.add_argument('--threshold', type=float, default=0.9885)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown task: {task}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class Flip:\n",
        "    _directions = ['horizontal', 'vertical']\n",
        "\n",
        "    def __init__(self, keys=['feature', 'label'], flip_ratio=0.5, direction='horizontal', **kwargs):\n",
        "        if direction not in self._directions:\n",
        "            raise ValueError(f'Direction {direction} is not supported.'\n",
        "                             f'Currently support ones are {self._directions}')\n",
        "        self.keys = keys\n",
        "        self.flip_ratio = flip_ratio\n",
        "        self.direction = direction\n",
        "\n",
        "    def __call__(self, results):\n",
        "        flip = np.random.random() < self.flip_ratio\n",
        "\n",
        "        if flip:\n",
        "            for key in self.keys:\n",
        "                if isinstance(results[key], list):\n",
        "                    for v in results[key]:\n",
        "                        mmcv.imflip_(v, self.direction)\n",
        "                else:\n",
        "                    mmcv.imflip_(results[key], self.direction)\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "class Rotation:\n",
        "    def __init__(self, keys=['feature', 'label'], axis=(0,1), rotate_ratio=0.5, **kwargs):\n",
        "        self.keys = keys\n",
        "        self.axis = {k:axis for k in keys} if isinstance(axis, tuple) else axis\n",
        "        self.rotate_ratio = rotate_ratio\n",
        "        self.direction = [0, -1, -2, -3]\n",
        "\n",
        "    def __call__(self, results):\n",
        "        rotate = np.random.random() < self.rotate_ratio\n",
        "\n",
        "        if rotate:\n",
        "            rotate_angle = self.direction[int(np.random.random()/(10.0/3.0))+1]\n",
        "            for key in self.keys:\n",
        "                if isinstance(results[key], list):\n",
        "                    for v in results[key]:\n",
        "                        results[key] = np.ascontiguousarray(np.rot90(v, rotate_angle, axes=self.axis[key]))\n",
        "                else:\n",
        "                    results[key] = np.ascontiguousarray(np.rot90(results[key], rotate_angle, axes=self.axis[key]))\n",
        "\n",
        "        return results\n",
        "\n",
        "class IterLoader:\n",
        "    def __init__(self, dataloader):\n",
        "        self._dataloader = dataloader\n",
        "        self.iter_loader = iter(self._dataloader)\n",
        "\n",
        "    def __next__(self):\n",
        "        try:\n",
        "            data = next(self.iter_loader)\n",
        "        except StopIteration:\n",
        "            time.sleep(2)\n",
        "            self.iter_loader = iter(self._dataloader)\n",
        "            data = next(self.iter_loader)\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._dataloader)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "class CongestionDataset(object):\n",
        "\n",
        "    def __init__(self, ann_file, dataroot, pipeline=None, test_mode=False):\n",
        "        self.ann_file = ann_file\n",
        "        self.dataroot = dataroot\n",
        "        self.test_mode = test_mode\n",
        "        self.pipeline = Compose(pipeline) if pipeline else None\n",
        "        self.data_infos = self.load_annotations()\n",
        "\n",
        "    def load_annotations(self):\n",
        "        data_infos = []\n",
        "        with open(self.ann_file, 'r') as fin:\n",
        "            for line in fin:\n",
        "                feature, label = line.strip().split(',')\n",
        "                data_infos.append(dict(\n",
        "                    feature_path=osp.join(self.dataroot, feature),\n",
        "                    label_path=osp.join(self.dataroot, label)\n",
        "                ))\n",
        "        return data_infos\n",
        "\n",
        "    def prepare_data(self, idx):\n",
        "        results = copy.deepcopy(self.data_infos[idx])\n",
        "        results['feature'] = np.load(results['feature_path'])\n",
        "        results['label'] = np.load(results['label_path'])\n",
        "\n",
        "        if self.pipeline:\n",
        "            results = self.pipeline(results)\n",
        "\n",
        "        feature = results['feature'].transpose(2, 0, 1).astype(np.float32)\n",
        "        label = results['label'].transpose(2, 0, 1).astype(np.float32)\n",
        "\n",
        "        return feature, label, results['label_path']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_infos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.prepare_data(idx)\n",
        "\n",
        "\n",
        "def build_dataset(opt):\n",
        "    opt = opt.copy()\n",
        "\n",
        "    aug_methods = {\n",
        "        'Flip': Flip(),\n",
        "        'Rotation': Rotation(**opt)\n",
        "    }\n",
        "\n",
        "    pipeline = None\n",
        "    if not opt.get('test_mode', False) and 'aug_pipeline' in opt:\n",
        "        pipeline = [aug_methods[name] for name in opt.pop('aug_pipeline')]\n",
        "\n",
        "    dataset = CongestionDataset(ann_file=opt.pop('ann_file'),dataroot=opt.pop('dataroot'),pipeline=pipeline,test_mode=opt.get('test_mode', False))\n",
        "\n",
        "    if opt.get('test_mode', False):\n",
        "        return DataLoader(dataset=dataset,batch_size=1,shuffle=False,num_workers=1)\n",
        "    else:\n",
        "        return IterLoader(\n",
        "            DataLoader(dataset=dataset,batch_size=opt.pop('batch_size'),shuffle=True,drop_last=True,num_workers=1,pin_memory=True)\n",
        "        )\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "amD0IEBujM6x"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generation_init_weights(module):\n",
        "    def init_func(m):\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1\n",
        "                                    or classname.find('Linear') != -1):\n",
        "\n",
        "            if hasattr(m, 'weight') and m.weight is not None:\n",
        "                nn.init.normal_(m.weight, 0.0, 0.02)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    module.apply(init_func)\n",
        "\n",
        "def load_state_dict(module, state_dict, strict=False, logger=None):\n",
        "    unexpected_keys = []\n",
        "    all_missing_keys = []\n",
        "    err_msg = []\n",
        "\n",
        "    metadata = getattr(state_dict, '_metadata', None)\n",
        "    state_dict = state_dict.copy()\n",
        "    if metadata is not None:\n",
        "        state_dict._metadata = metadata\n",
        "\n",
        "    def load(module, prefix=''):\n",
        "        local_metadata = {} if metadata is None else metadata.get(\n",
        "            prefix[:-1], {})\n",
        "        module._load_from_state_dict(state_dict, prefix, local_metadata, True,\n",
        "                                     all_missing_keys, unexpected_keys,\n",
        "                                     err_msg)\n",
        "        for name, child in module._modules.items():\n",
        "            if child is not None:\n",
        "                load(child, prefix + name + '.')\n",
        "\n",
        "    load(module)\n",
        "    load = None\n",
        "\n",
        "    missing_keys = [\n",
        "        key for key in all_missing_keys if 'num_batches_tracked' not in key\n",
        "    ]\n",
        "\n",
        "    if unexpected_keys:\n",
        "        err_msg.append('unexpected key in source '\n",
        "                       f'state_dict: {\", \".join(unexpected_keys)}\\n')\n",
        "    if missing_keys:\n",
        "        err_msg.append(\n",
        "            f'missing keys in source state_dict: {\", \".join(missing_keys)}\\n')\n",
        "\n",
        "    if len(err_msg) > 0:\n",
        "        err_msg.insert(\n",
        "            0, 'The model and loaded state dict do not match exactly\\n')\n",
        "        err_msg = '\\n'.join(err_msg)\n",
        "        if strict:\n",
        "            raise RuntimeError(err_msg)\n",
        "        elif logger is not None:\n",
        "            logger.warning(err_msg)\n",
        "        else:\n",
        "            print(err_msg)\n",
        "    return missing_keys\n",
        "\n",
        "class conv(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=True):\n",
        "        super(conv, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(dim_out, dim_out, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias),\n",
        "            nn.InstanceNorm2d(dim_out, affine=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class upconv(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super(upconv, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "                nn.ConvTranspose2d(dim_in, dim_out, 4, 2, 1),\n",
        "                nn.InstanceNorm2d(dim_out, affine=True),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_dim=3, out_dim=32):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.c1 = conv(in_dim, 32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.c2 = conv(32, 64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        self.c3 = nn.Sequential(\n",
        "                nn.Conv2d(64, out_dim, 3, 1, 1),\n",
        "                nn.BatchNorm2d(out_dim),\n",
        "                nn.Tanh()\n",
        "                )\n",
        "\n",
        "    def init_weights(self):\n",
        "        generation_init_weights(self)\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "        h1 = self.c1(input)\n",
        "        h2 = self.pool1(h1)\n",
        "        h3 = self.c2(h2)\n",
        "        h4 = self.pool2(h3)\n",
        "        h5 = self.c3(h4)\n",
        "        return h5, h2  # shortpath from 2->7\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, out_dim=2, in_dim=32):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.conv1 = conv(in_dim, 32)\n",
        "        self.upc1 = upconv(32, 16)\n",
        "        self.conv2 = conv(16, 16)\n",
        "        self.upc2 = upconv(32+16, 4)\n",
        "        self.conv3 =  nn.Sequential(\n",
        "                nn.Conv2d(4, out_dim, 3, 1, 1),\n",
        "                nn.Sigmoid()\n",
        "                )\n",
        "\n",
        "    def init_weights(self):\n",
        "        generation_init_weights(self)\n",
        "\n",
        "    def forward(self, input):\n",
        "        feature, skip = input\n",
        "        d1 = self.conv1(feature)\n",
        "        d2 = self.upc1(d1)\n",
        "        d3 = self.conv2(d2)\n",
        "        d4 = self.upc2(torch.cat([d3, skip], dim=1))\n",
        "        output = self.conv3(d4)  # shortpath from 2->7\n",
        "        return output\n",
        "\n",
        "\n",
        "class GPDL(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels=3,\n",
        "                 out_channels=2,\n",
        "                 **kwargs):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(in_dim=in_channels)\n",
        "        self.decoder = Decoder(out_dim=out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        return self.decoder(x)\n",
        "\n",
        "    def init_weights(self, pretrained=None, pretrained_transfer=None, strict=False, **kwargs):\n",
        "        if isinstance(pretrained, str):\n",
        "            new_dict = OrderedDict()\n",
        "            weight = torch.load(pretrained, map_location='cpu')['state_dict']\n",
        "            for k in weight.keys():\n",
        "                new_dict[k] = weight[k]\n",
        "            load_state_dict(self, new_dict, strict=strict, logger=None)\n",
        "        elif pretrained is None:\n",
        "            generation_init_weights(self)\n",
        "        else:\n",
        "            raise TypeError(\"'pretrained' must be a str or None. \"\n",
        "                            f'But received {type(pretrained)}.')\n",
        "def build_model(opt):\n",
        "    model = GPDL()\n",
        "    model.init_weights(**opt)\n",
        "    if opt['test_mode']:\n",
        "        model.eval()\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "6-FbssPIYx-A"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_loss(loss, reduction):\n",
        "    if reduction == 'none':\n",
        "        return loss\n",
        "    elif reduction == 'mean':\n",
        "        return loss.mean()\n",
        "    elif reduction == 'sum':\n",
        "        return loss.sum()\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid reduction: {reduction}\")\n",
        "\n",
        "\n",
        "def mask_reduce_loss(loss, weight=None, reduction='mean', sample_wise=False):\n",
        "    if weight is not None:\n",
        "        loss = loss * weight\n",
        "\n",
        "    if reduction == 'sum':\n",
        "        return loss.sum()\n",
        "\n",
        "    if reduction == 'mean':\n",
        "        eps = 1e-12\n",
        "        if weight is None:\n",
        "            return loss.mean()\n",
        "\n",
        "        if sample_wise:\n",
        "            # Normalize per-sample\n",
        "            weight_sum = weight.sum(dim=[1, 2, 3], keepdim=True)\n",
        "            return (loss / (weight_sum + eps)).sum() / weight.size(0)\n",
        "        else:\n",
        "            return loss.sum() / (weight.sum() + eps)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def masked_loss(loss_func):\n",
        "    @functools.wraps(loss_func)\n",
        "    def wrapper(pred, target, weight=None, reduction='mean', sample_wise=False):\n",
        "        loss = loss_func(pred, target)\n",
        "        return mask_reduce_loss(loss, weight, reduction, sample_wise)\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "@masked_loss\n",
        "def mse_loss(pred, target):\n",
        "    return F.mse_loss(pred, target, reduction='none')\n",
        "\n",
        "\n",
        "class MSELoss(nn.Module):\n",
        "    def __init__(self, loss_weight=100.0, reduction='mean', sample_wise=False):\n",
        "        super().__init__()\n",
        "        self.loss_weight = loss_weight\n",
        "        self.reduction = reduction\n",
        "        self.sample_wise = sample_wise\n",
        "\n",
        "    def forward(self, pred, target, weight=None):\n",
        "        return self.loss_weight * mse_loss(\n",
        "            pred,\n",
        "            target,\n",
        "            weight=weight,\n",
        "            reduction=self.reduction,\n",
        "            sample_wise=self.sample_wise\n",
        "        )\n",
        "def build_loss(opt):\n",
        "    # opt = opt.copy()\n",
        "    loss_type = \"MSELoss\"\n",
        "    loss_cls = globals()[loss_type]\n",
        "\n",
        "    return torch.nn.MSELoss()\n",
        "\n"
      ],
      "metadata": {
        "id": "b3KrjIAqa2R5"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def checkpoint(model, epoch, save_path):\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    model_out_path = f\"./{save_path}/model_iters_{epoch}.pth\"\n",
        "    torch.save({'state_dict': model.state_dict()}, model_out_path)\n",
        "\n",
        "\n",
        "class CosineRestartLr(object):\n",
        "\n",
        "    def __init__(self,\n",
        "                 base_lr,\n",
        "                 periods,\n",
        "                 restart_weights = [1],\n",
        "                 min_lr = None,\n",
        "                 min_lr_ratio = None):\n",
        "        self.periods = periods\n",
        "        self.min_lr = min_lr\n",
        "        self.min_lr_ratio = min_lr_ratio\n",
        "        self.restart_weights = restart_weights\n",
        "        super().__init__()\n",
        "\n",
        "        self.cumulative_periods = [\n",
        "            sum(self.periods[0:i + 1]) for i in range(0, len(self.periods))\n",
        "        ]\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "\n",
        "    def annealing_cos(self, start: float,\n",
        "                    end: float,\n",
        "                    factor: float,\n",
        "                    weight: float = 1.) -> float:\n",
        "        cos_out = math.cos(math.pi * factor) + 1\n",
        "        return end + 0.5 * weight * (start - end) * cos_out\n",
        "\n",
        "    def get_position_from_periods(self, iteration: int, cumulative_periods):\n",
        "        for i, period in enumerate(cumulative_periods):\n",
        "            if iteration < period:\n",
        "                return i\n",
        "        raise ValueError(f'Current iteration {iteration} exceeds '\n",
        "                        f'cumulative_periods {cumulative_periods}')\n",
        "\n",
        "\n",
        "    def get_lr(self, iter_num, base_lr: float):\n",
        "        target_lr = self.min_lr  # type:ignore\n",
        "\n",
        "        idx = self.get_position_from_periods(iter_num, self.cumulative_periods)\n",
        "        current_weight = self.restart_weights[idx]\n",
        "        nearest_restart = 0 if idx == 0 else self.cumulative_periods[idx - 1]\n",
        "        current_periods = self.periods[idx]\n",
        "\n",
        "        alpha = min((iter_num - nearest_restart) / current_periods, 1)\n",
        "        return self.annealing_cos(base_lr, target_lr, alpha, current_weight)\n",
        "\n",
        "\n",
        "    def _set_lr(self, optimizer, lr_groups):\n",
        "        if isinstance(optimizer, dict):\n",
        "            for k, optim in optimizer.items():\n",
        "                for param_group, lr in zip(optim.param_groups, lr_groups[k]):\n",
        "                    param_group['lr'] = lr\n",
        "        else:\n",
        "            for param_group, lr in zip(optimizer.param_groups,\n",
        "                                        lr_groups):\n",
        "                param_group['lr'] = lr\n",
        "\n",
        "    def get_regular_lr(self, iter_num):\n",
        "        return [self.get_lr(iter_num, _base_lr) for _base_lr in self.base_lr]  # iters\n",
        "\n",
        "    def set_init_lr(self, optimizer):\n",
        "        for group in optimizer.param_groups:  # type: ignore\n",
        "            group.setdefault('initial_lr', group['lr'])\n",
        "            self.base_lr = [group['initial_lr'] for group in optimizer.param_groups  # type: ignore\n",
        "        ]\n",
        "\n",
        "\n",
        "def train(arg_dict):\n",
        "\n",
        "    device = torch.device(\"cpu\" if arg_dict.get(\"cpu\", False) else \"cuda\")\n",
        "    print(\"Using device:\", device)\n",
        "\n",
        "    os.makedirs(arg_dict[\"save_path\"], exist_ok=True)\n",
        "\n",
        "    with open(os.path.join(arg_dict[\"save_path\"], \"arg.json\"), \"w\") as f:\n",
        "        json.dump(arg_dict, f, indent=4)\n",
        "\n",
        "    arg_dict[\"ann_file\"] = arg_dict[\"ann_file_train\"]\n",
        "    arg_dict[\"test_mode\"] = False\n",
        "\n",
        "\n",
        "    print(\"===> Loading datasets\")\n",
        "    dataset = build_dataset(arg_dict)\n",
        "\n",
        "    if not hasattr(dataset, \"__iter__\") or not hasattr(dataset, \"__len__\"):\n",
        "        dataset = DataLoader(dataset, batch_size=arg_dict[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    print(\"===> Building model\")\n",
        "    model = build_model(arg_dict).to(device)\n",
        "\n",
        "    criterion = build_loss(arg_dict)\n",
        "\n",
        "    optimizer = optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=arg_dict[\"lr\"],\n",
        "        betas=(0.9, 0.999),\n",
        "        weight_decay=arg_dict[\"weight_decay\"]\n",
        "    )\n",
        "\n",
        "    cosine_lr = CosineRestartLr(\n",
        "        arg_dict[\"lr\"],\n",
        "        [arg_dict[\"max_iters\"]],\n",
        "        [1],\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "    cosine_lr.set_init_lr(optimizer)\n",
        "\n",
        "\n",
        "    iter_num = 0\n",
        "    epoch_loss = 0.0\n",
        "    print_freq = 100\n",
        "    save_freq = 10000\n",
        "\n",
        "    while iter_num < arg_dict[\"max_iters\"]:\n",
        "        with tqdm(total=print_freq) as bar:\n",
        "          for feature, label, _ in dataset:\n",
        "                feature = feature.to(device)\n",
        "                label = label.to(device)\n",
        "\n",
        "                lr = cosine_lr.get_regular_lr(iter_num)\n",
        "                cosine_lr._set_lr(optimizer, lr)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                prediction = model(feature)\n",
        "                loss = criterion(prediction, label)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "                iter_num += 1\n",
        "                bar.update(1)\n",
        "\n",
        "                if iter_num % print_freq == 0:\n",
        "                    break\n",
        "\n",
        "        print(\n",
        "            f\"===> Iters[{iter_num}/{arg_dict['max_iters']}]: \"\n",
        "            f\"Loss: {epoch_loss / print_freq:.4f}\"\n",
        "        )\n",
        "\n",
        "        if iter_num % save_freq == 0:\n",
        "            checkpoint(model, iter_num, arg_dict[\"save_path\"])\n",
        "\n",
        "        epoch_loss = 0.0\n"
      ],
      "metadata": {
        "id": "PAZY2_Ox_85C"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    \"task\": \"congestion_gpdl\",\n",
        "    \"save_path\": \"work_dir/\",\n",
        "    \"ann_file_train\": \"/content/train_N28.csv\",\n",
        "    \"dataroot\": \"/content/drive/MyDrive/congestion\",\n",
        "    \"lr\": 1e-4,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"batch_size\": 1,\n",
        "    \"max_iters\": 200,\n",
        "    \"cpu\": True,\n",
        "}\n",
        "\n",
        "train(args)\n"
      ],
      "metadata": {
        "id": "YBqQF4H4eHp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b82a8d05-a87b-4b09-d95f-b71e59c5983e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "===> Loading datasets\n",
            "===> Building model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:03<00:00,  1.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===> Iters[100/200]: Loss: 0.1381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|▏         | 2/100 [00:00<00:42,  2.33it/s]"
          ]
        }
      ]
    }
  ]
}